{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing & Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_batch_size, test_batch_size):\n",
    "    # Data in train set and test set are [im_tensor, label]. im_tensor size - 1x32x32 (gray scale, 32x32 pixels)\n",
    "    trainset = datasets.MNIST('../Dataset/', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                               transforms.Resize((32, 32)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "\n",
    "    # Split train data to validation set and train set (20-80%)\n",
    "    val_set_size = int(0.2 * len(trainset))\n",
    "    trainset, valset = torch.utils.data.random_split(trainset, [len(trainset) - val_set_size, val_set_size])\n",
    "\n",
    "    testset = datasets.MNIST('../Dataset/', train=False,\n",
    "                              transform=transforms.Compose([\n",
    "                              transforms.Resize((32, 32)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
    "    val_loader   = torch.utils.data.DataLoader(valset,   batch_size=train_batch_size, shuffle=False)\n",
    "    test_loader  = torch.utils.data.DataLoader(testset,  batch_size=test_batch_size,  shuffle=False)\n",
    "\n",
    "    return trainset, train_loader, valset, val_loader, testset, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, Cin, Cout1, Cout2, Cout3, feat4, featOut):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.C1   = nn.Sequential(nn.Conv2d(in_channels=Cin, out_channels=Cout1, kernel_size=5),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.C2   = nn.Sequential(nn.Conv2d(in_channels=Cout1, out_channels=Cout2, kernel_size=5),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.C3   = nn.Sequential(nn.Conv2d(in_channels=Cout2, out_channels=Cout3, kernel_size=5),\n",
    "                                nn.ReLU())\n",
    "        \n",
    "        self.fc1  = nn.Sequential(nn.Linear(Cout3,feat4),\n",
    "                                 nn.ReLU())\n",
    "        \n",
    "        self.fc2  = nn.Linear(feat4, featOut)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.2) \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.C1(input)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.C2(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.C3(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # flatten all dimensions except the batch dimension\n",
    "        x = torch.flatten(x, 1) \n",
    "        \n",
    "        x = self.fc1(x) \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device, epoch):\n",
    "\n",
    "    model.train()\n",
    "    train_loss  = 0\n",
    "    tot_correct = 0\n",
    "\n",
    "    writer = SummaryWriter('Events/runs')\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # Load batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero gradients \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate predictions\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss   = criterion(output, target)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights \n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Calculate prediction accuracy\n",
    "        train_loss  += loss.data  # sum up batch loss\n",
    "        \n",
    "        preds        = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability   \n",
    "        correct      = preds.eq(target.data.view_as(preds)).cpu().sum().item() # sum up batch correct  \n",
    "        tot_correct += correct\n",
    "        \n",
    "        accuracy     = 100. * correct / len(target)    \n",
    "       \n",
    "        # print log\n",
    "        if batch_idx % 100 == 0:         \n",
    "            print(f'Train set, Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
    "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n",
    "                  f'Loss: {loss.data:.4f}\\t'\n",
    "                  f' Accuracy: {accuracy:.3f}')\n",
    "\n",
    "            # ...log the running loss, accuracy and bounding \n",
    "            writer.add_scalar(tag='training loss',\n",
    "                              scalar_value = loss.data,\n",
    "                              global_step  = batch_idx+((epoch-1)*100*4))\n",
    "\n",
    "            writer.add_scalar(tag='Accuracy',\n",
    "                              scalar_value = accuracy,\n",
    "                              global_step  = batch_idx+((epoch-1)*100*4))\n",
    "        \n",
    "\n",
    "    train_loss    /= len(train_loader.dataset)\n",
    "    train_accuracy = 100. * tot_correct / len(train_loader.dataset)\n",
    "    \n",
    "    writer.close()   \n",
    "    return train_loss, train_accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct  = 0\n",
    "\n",
    "    for data, target in tqdm(val_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output       = model(data)\n",
    "        val_loss    += criterion(output, target).data  # sum up batch loss\n",
    "        \n",
    "        preds    = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += preds.eq(target.data.view_as(preds)).cpu().sum().item()\n",
    "        \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy  = 100. * correct / len(val_loader.dataset)\n",
    "\n",
    "    print('\\nValidation set: average loss: {:.6f}, accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "          val_loss, correct, len(val_loader.dataset), accuracy))\n",
    "    \n",
    "    return val_loss, accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "train_batch_size = 64\n",
    "test_batch_size  = 1000\n",
    "\n",
    "train_set, train_loader, val_set, val_loader, test_set, test_loader = load_data(train_batch_size, test_batch_size)\n",
    "print(f'data shape: train {len(train_set)}, val {len(val_set)}, test {len(test_set)}\\n\\n')\n",
    "\n",
    "# Set training parameters\n",
    "device      = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on: {device}\\n\\n')\n",
    "num_epoches = 10\n",
    "\n",
    "# Model parameters\n",
    "Cin     = 1     # Gray scale - 1 channel\n",
    "Cout1   = 6     # Output channels for 1st Convolution layer\n",
    "Cout2   = 16    # Output channels for 2nd Convolution layer\n",
    "Cout3   = 120   # Output channels for 3rd Convolution layer\n",
    "feat4   = 84    # Output features for 1st Fully Connected layer\n",
    "featOut = 10    # Output features for 2nd Fully Connected layer\n",
    "\n",
    "model = LeNet5(Cin, Cout1, Cout2, Cout3, feat4, featOut).to(device)\n",
    "\n",
    "# Set optimizer & criterion\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate model\n",
    "for epoch in range(num_epoches):\n",
    "    train(model, train_loader, optimizer, criterion, device, epoch)\n",
    "    eval(model, val_loader, criterion, device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model accuracy based on test data set:\\n\\n')\n",
    "loss, accuracy = eval(model, test_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7d800ec1281210af2e9ff328fbdec3c68c6a0a400632e06fb16360d57c70088"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
